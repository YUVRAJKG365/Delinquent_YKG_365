📊 Customer Delinquency Prediction System – AI-Powered Risk Assessment & Fairness-Aware Modeling
This project presents a production-ready, end-to-end AI system that predicts customer delinquency using behavioral, financial, and demographic indicators – empowered by machine learning, advanced feature engineering, and responsible AI practices. Built to support real-world decision-making for financial institutions, this project also integrates model fairness audits and bias mitigation using the Fairlearn framework.

🚀 Key Features
✅ Interactive Streamlit App for real-time risk prediction and recommendation

🔍 AI Model Training Pipeline using Logistic Regression and Random Forest with data balancing

📈 Explainable Risk Factors like missed payments, debt-to-income ratio, credit utilization

🧠 Post-processing with Fairlearn to ensure fairness across employment and location groups

📊 Bias Auditing Dashboard – Demographic Parity, Equalized Odds, and Group Metrics

💡 SMART Business Recommendations based on model findings

🔐 Ethical AI Considerations – explainability, transparency, and fairness-by-design


🧪 Model Performance
Metric	Value
Accuracy	98.8%
Precision	98.8%
Recall (TPR)	98.8%
Demographic Parity Δ	0.237
Equalized Odds Δ	0.091

✅ Post-processing with Fairlearn reduces bias while maintaining high accuracy and predictive power.

🎯 Business Use Case
This AI solution helps financial institutions like Geldium optimize their collections strategy by:

Identifying delinquency risk 3 months in advance

Enabling targeted outreach and support for high-risk customers

Reducing collections cost by up to 35%

Improving ethical and regulatory compliance in credit decisioning

🛠️ Tech Stack
Python (Pandas, NumPy, scikit-learn, joblib)

Streamlit – Interactive app for predictions

Fairlearn – Fairness auditing and bias mitigation

imbalanced-learn – Class balancing (upsampling)

Jupyter Notebooks – Training and exploration


🤖 Model Ethics & Explainability
This project includes a full section on responsible AI practices, including:

📊 Bias detection across sensitive features (Employment, Location)

🧩 Post-training mitigation using Equalized Odds

📣 Transparent predictions with clear justification

🔄 Explainable outputs designed for non-technical stakeholders

📌 Acknowledgment
Developed by Yuvraj Kumar Gond as part of a real-world applied AI challenge, with a focus on data science, fairness, and human-centered design.
